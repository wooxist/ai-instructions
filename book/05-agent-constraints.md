# 5장. 역할(Agent)과 제약(Constraint) 설계

**Part 2: 인스트럭션 시스템 설계와 평가**

**목적:** 단일 프롬프트의 한계를 이해하고, 이를 극복하기 위해 역할과 제약이 명확한 '에이전트'를 설계하여 안정적이고 효율적인 자동화 시스템을 구축하는 방법을 배웁니다.

### 이 장에서 배우는 것
- 단일 프롬프트가 가진 본질적 한계(컨텍스트 윈도우, 할루시네이션 등)와 체계적 자동화의 기회.
- 문제 해결 단위로서 '에이전트'의 개념과 역할의 범위를 설정하는 기준.
- AI의 한계를 극복하고 업무 효율을 높이기 위한 에이전트 설계 방법론(역할, 제약, 페르소나 등).
- 4장에서 배운 메타 원칙(SoC, 산출물 중심 등)이 에이전트 설계에 어떻게 적용되는지.

## 5.1 왜 '에이전트'를 설계해야 하는가?

지금까지 우리는 좋은 인스트럭션을 만드는 법을 배웠습니다. 하지만 작업이 복잡해지면, 아무리 잘 만든 인스트럭션이라도 하나의 거대한 프롬프트만으로는 한계에 부딪힙니다. 이는 마치 모든 일을 혼자 처리하려는 '만능 일꾼'에게 의존하는 것과 같습니다. 처음에는 효율적인 것 같지만, 일이 복잡해질수록 실수가 잦아지고 결국 아무것도 제대로 해내지 못하게 됩니다.

'에이전트 설계'는 '만능 일꾼'의 한계를 인정하는 것에서 시작합니다. 대신 각자의 전문 분야를 가진 '전문가 팀'을 구성하는 접근법입니다. 여기에는 두 가지 핵심 동기가 있습니다.

### 5.1.1 [필요의 측면] AI의 본질적 한계 극복

AI 모델은 근본적인 기술적 한계를 가지고 있으며, 이는 단일 프롬프트의 규모가 커질수록 심각한 문제로 나타납니다.

#### 컨텍스트 윈도우와 지시부분 상실
AI가 한 번에 처리할 수 있는 정보량인 컨텍스트 윈도우(Context Window)[^1]는 제한적입니다. 지시가 길어지면 AI는 앞부분의 중요한 규칙을 잊어버리거나(Lost in the Middle)[^2], 전체적인 일관성을 잃어버립니다.

#### 할루시네이션(Hallucination)[^3]
AI는 사실이 아닌 정보를 그럴듯하게 지어내는 경향이 있습니다. 프롬프트가 복잡하고 여러 소스의 정보를 다룰수록, 할루시네이션이 발생할 가능성은 커지고 결과물의 신뢰도는 떨어집니다.

#### 비일관성과 변동성
동일한 프롬프트에 대해서도 AI는 미세하게 다른 결과물을 생성합니다. 이는 일관된 결과가 필수적인 업무 자동화에 큰 장애물이 됩니다.

#### 유지보수 및 확장성
수백, 수천 줄의 거대 프롬프트는 '스파게티 코드'와 같습니다. 작은 수정이 어디에 영향을 미칠지 예측하기 어렵고, 일부 로직만 떼어내 재사용하거나 다른 사람과 협업하기도 거의 불가능합니다.

#### 보안 취약점
외부 데이터나 사용자 입력에 숨겨진 악의적인 지시(프롬프트 인젝션)[^5]에 취약하여, 의도치 않은 행동을 하거나 민감 정보를 유출할 수 있습니다.

### 5.1.2 [기회의 측면] 체계적인 업무 자동화

반대로, AI의 한계를 인정하고 새로운 접근법을 고민하는 것은 복잡한 업무를 자동화할 엄청난 기회를 열어줍니다.

#### 업무의 모듈화 및 재사용
복잡한 업무를 '이메일 분류', '데이터 추출', '보고서 초안 작성' 등 잘게 쪼개고, 각 단계를 전담하는 에이전트를 만들면 재사용성이 극대화됩니다. '데이터 추출 에이전트'는 고객 분석뿐만 아니라 재무 분석 워크플로우에도 레고 블록처럼 가져다 쓸 수 있습니다.

#### 프로세스 효율성과 확장성
표준화된 에이전트들을 조립하여 '월간 보고서 발행'과 같은 복잡한 워크플로우를 빠르고 일관되게 실행할 수 있습니다. 이는 개인의 생산성을 넘어, 팀 전체, 조직 전체로 확장 가능한 자동화 시스템의 기반이 됩니다.

#### 명확한 책임과 협업
각 에이전트가 맡은 역할과 책임이 명확해져, 문제 발생 시 원인 파악이 쉽습니다. 이는 여러 에이전트 간의 협업은 물론, 사람과 AI의 협업 구조를 명확하게 만드는 효과도 있습니다.

## 5.2 해결 원칙과 방법론: 단일 책임을 갖는 '에이전트'로 분할하라

앞서 제기된 한계와 기회에 대한 가장 효과적인 해결책은 4장에서 배운 **관심사 분리(SoC, Separation of Concerns)** 원칙을 인스트럭션에 적용하는 것입니다. 즉, 모든 것을 처리하려는 하나의 만능 프롬프트를 버리고, **명확하게 정의된 단일 책임(Single Responsibility)을 가진 전문가 '에이전트'들의 시스템**으로 문제를 해결하는 것입니다.

'에이전트'는 단순히 프롬프트를 나누는 것을 넘어, 특정 역할, 책임, 제약 조건을 가진 독립적인 실행 단위입니다. 예를 들어, '고객 리뷰 분석'이라는 큰 작업은 다음과 같은 에이전트들의 협력으로 수행될 수 있습니다.

1.  **분류 에이전트:** 고객 리뷰를 '긍정', '부정', '문의'로 분류한다.
2.  **핵심 추출 에이전트:** 각 리뷰에서 핵심 불만/칭찬 사항을 추출한다.
3.  **초안 작성 에이전트:** 추출된 내용을 바탕으로 주간 보고서 초안을 작성한다.
4.  **검증 에이전트:** 초안에 할루시네이션이나 왜곡은 없는지 원본 리뷰와 비교하여 검증한다.

이처럼 에이전트 단위로 작업을 분할하면, 각 단계가 단순해져 AI의 한계를 회피하고, 각 에이전트를 독립적으로 개선하거나 재사용하여 업무 효율을 극대화할 수 있습니다.

그렇다면 구체적으로 어떻게 에이전트를 설계해야 할까요? 5.1절에서 제기된 각 문제점들을 해결하기 위한 구체적인 설계 방법론을 살펴보겠습니다.

### 5.2.1 [할루시네이션 문제 해결] → 근거 기반 '책임'과 '검증 에이전트' 설계

할루시네이션을 줄이고 신뢰도를 높이기 위해, 에이전트에게 '책임'을 부여하고 '역할'을 분리해야 합니다. 이는 마치 언론사의 '기자'와 '팩트 체커'처럼 역할을 나누는 것과 같습니다. 이는 4장의 **투명성 및 추적 가능성(Transparency & Traceability)** 원칙과 연결됩니다.

- **근거 기반 책임 부여:** 모든 '생성' 관련 에이전트에게 "제공된 문서나 데이터 내에서만 정보를 찾아야 하며, 모든 주장은 반드시 출처를 명시해야 한다"는 핵심 **책임**을 부여하고, "추측은 엄격히 금지된다"는 **제약**을 설정합니다.
- **지식 출처 한정(SSOT) 적용:** 4장의 SSOT 중 "지식 출처 한정" 원칙을 에이전트 **제약**으로 구체화합니다. 허용된 자료원(코퍼스/문서 집합)만 사용하도록 명시하고, 결과에는 반드시 **출처 식별자와 근거 인용(citations)**을 포함하게 합니다. 도구·검색 구현 방법은 [11장. 도구와 프레임워크](11-tools.md) 참고.
- **검증 역할 분리:** 더 나아가, 콘텐츠를 생성하는 '생성 에이전트'와 별도로, 생성된 내용의 사실 여부를 확인하는 '검증 에이전트' **역할**을 워크플로우에 포함시킵니다. 이는 사람의 검토 부담을 크게 줄여주면서도 신뢰도를 높이는 효과적인 방법입니다.

### 5.2.2 [컨텍스트 윈도우 문제 해결] → '역할 분할'과 '명시적 제약' 설계

긴 지시를 잊어버리는 문제를 해결하기 위해, 복잡한 작업을 여러 에이전트의 **역할**로 나누어 컨텍스트 윈도우의 부담을 근본적으로 줄여야 합니다. 이는 마치 프로젝트 매니저가 거대한 프로젝트를 여러 개의 작은 태스크로 나누어 각 팀원에게 할당하는 것과 같습니다. 이는 **관심사 분리(SoC)** 원칙의 가장 중요한 적용 사례입니다. 또한, RAG(Retrieval-Augmented Generation)[^4] 기술을 활용해 필요한 정보만 동적으로 주입하는 것도 효과적입니다.

- **단일 책임 역할 분할:** 하나의 에이전트가 하나의 단순한 작업만 처리하도록 역할을 잘게 나눕니다. 이렇게 하면 각 에이전트의 인스트럭션이 짧고 명확해져, AI가 지시를 잊어버릴 가능성이 현저히 줄어듭니다.
- **망각 방지를 위한 제약 설정:** 그럼에도 불구하고 중요한 규칙이 있다면, "인스트럭션의 가장 중요한 규칙은 맨 마지막에 요약되어 있으니 반드시 다시 참조하라"와 같은 **제약**을 추가하여 망각을 방지할 수 있습니다.

### 5.2.3 [비일관성 문제 해결] → 명확한 '페르소나'와 '구조화된 출력' 설계

결과물의 변동성을 제어하고 일관성을 확보하기 위해, 에이전트의 **페르소나**를 정의하고 **산출물 중심(Output-Driven)** 원칙에 따라 출력 형식을 강제해야 합니다.

- **구체적인 페르소나 부여:** "당신은 20년차 변호사이며, 법률 용어를 정확히 사용하되 고객에게는 쉬운 용어로 설명해야 한다"와 같이 구체적인 **페르소나**를 부여하면, AI의 말투, 관점, 용어 선택이 일관되게 유지됩니다.
- **구조화된 출력 형식 강제:** 자유로운 서술 형식 대신, JSON 스키마, YAML, 표 등 명확한 구조를 가진 출력 형식을 **제약**으로 설정하면, AI가 생성하는 결과의 변동성이 크게 줄어듭니다. 이는 6장에서 더 상세히 다룹니다.

### 5.2.4 [유지보수/확장성 문제 해결] → '모듈화'된 에이전트 설계

'스파게티 프롬프트' 문제를 해결하고 재사용성을 높이기 위해, 각 에이전트를 독립적인 **모듈**로 설계해야 합니다. 이는 **관심사 분리(SoC)**와 **MECE** 원칙을 동시에 적용하는 것입니다.

- **독립적인 모듈 설계:** 각 에이전트를 언제든 교체하거나 개선할 수 있는 독립적인 부품처럼 설계합니다. 예를 들어, '보고서 초안 작성 에이전트'의 성능이 만족스럽지 않다면, 다른 부분에 영향을 주지 않고 해당 에이전트만 교체하거나 업데이트할 수 있습니다.
- **MECE 기반 역할 분배:** 여러 에이전트의 역할을 나눌 때, 서로 중복되지 않고(Mutually Exclusive) 전체 워크플로우를 빠짐없이(Collectively Exhaustive) 처리하도록 **MECE** 원칙에 따라 역할을 분배하면, 시스템 전체의 복잡도가 관리되고 예측 가능성이 높아집니다. 이렇게 설계된 에이전트들은 다른 워크플로우에서 레고 블록처럼 쉽게 재사용될 수 있어, 전체 시스템의 확장성과 생산성을 극대화합니다.

## 5.3 에이전트 범위 설계를 위한 실용 가이드

"내 에이전트를 얼마나 크게, 또는 작게 만들어야 하는가?" 이 질문에 답하기 위해, 우리는 먼저 물리적 제약을 확인하고, 그 다음에 설계 원칙을 적용하는 2단계 접근법을 사용합니다.

### 5.3.1 1단계: 물리적 제약 확인 - 컨텍스트 윈도우 크기

에이전트 설계의 가장 첫 번째이자 가장 중요한 기준은 **"인스트럭션과 데이터가 모델의 컨텍스트 윈도우 안에 들어가는가?"** 입니다.

#### 토큰 예산 개념

모델의 전체 컨텍스트 윈도우를 '총 예산'으로 보고, 이 예산을 '인스트럭션', '입력 데이터', '출력 데이터' 세 항목에 배분합니다.

`총 예산 (모델 컨텍스트 윈도우) = 인스트럭션 토큰 + 입력 데이터 토큰 + 출력 데이터 토큰`

#### 경험 법칙 (Rules of Thumb)

**1. 인스트럭션-데이터 비율**
- 에이전트의 자체 인스트럭션(역할, 처리 방법, 제약 등)이 **전체 컨텍스트 윈도우의 20~30%를 넘지 않도록** 설계합니다.
- 실제 작업에 필요한 입력 데이터와 AI가 생성할 출력물을 위한 충분한 공간(70~80%)을 확보해야 합니다.
- 예: 128k 컨텍스트 윈도우를 가진 모델이라면, 에이전트 인스트럭션 자체는 약 25k~38k 토큰 미만으로 유지하는 것이 이상적입니다.

**2. 분리 결정 기준**
- `[에이전트의 인스트럭션 총 길이] + [평균 입력 데이터의 길이] > [모델 컨텍스트 윈도우의 70%]` 라면, 에이전트 분리를 강력하게 고려해야 합니다.
- 컨텍스트 윈도우가 꽉 차면 AI의 성능이 급격히 저하되기 때문입니다.

**3. 최소 기능 인스트럭션 (Minimum Viable Instruction)**
- 처음부터 완벽하고 긴 인스트럭션을 만들기보다, 작업을 수행할 수 있는 가장 짧고 간단한 버전으로 시작하세요.
- 결과물의 품질이 만족스럽지 않을 때만, 페르소나, 예시, 제약 조건 등을 추가하며 점진적으로 길이를 늘려나갑니다. (**점진적 개선** 원칙)

**4. 모델의 성능 고려**
- **고성능 모델 (예: GPT-4, Claude 3 Opus):** 더 적은 토큰의 간결한 지시로도 의도를 잘 파악하는 경향이 있습니다.
- **고속/저비용 모델 (예: GPT-3.5, Llama 3 8B):** 원하는 품질을 얻기 위해 더 상세하고 명시적인 지시와 예시가 필요하여, 인스트럭션이 더 길어질 수 있습니다.

**5. 비용을 제약 조건으로 활용**
- 비용이 중요한 요소라면, 이는 에이전트 설계의 강력한 제약 조건이 됩니다.
- 비싼 모델을 사용하면서 긴 인스트럭션을 유지하기 어렵다면, 자연스럽게 하나의 복잡한 에이전트를 두 개의 더 작고 저렴한 에이전트로 분리하게 될 것입니다.

**결론:** '적정 크기'란 **"원하는 결과물을 안정적으로 만들어내는 가장 작은 크기"**를 의미하며, 이는 엄격한 공식이 아닌 반복적인 실험과 측정을 통해 찾아가는 값입니다.

> **참고: 사용 중인 AI 모델의 컨텍스트 윈도우 크기 확인 방법**
>
> 1. **공식 문서 참조**
>    - Anthropic (Claude): Claude 3.5 Sonnet/Opus/Haiku - 200K 토큰[^8]
>    - OpenAI (GPT): GPT-4 Turbo - 128K, GPT-4 - 8K/32K, GPT-3.5 Turbo - 16K 토큰[^9]
>    - Google (Gemini): Gemini 1.5 Pro - 2M, Gemini 1.5 Flash - 1M 토큰[^10]
>
> 2. **API 응답 확인** (프로그래밍 방식)
>    ```python
>    # OpenAI 예시
>    response = client.chat.completions.create(...)
>    print(response.usage.total_tokens)  # 사용된 총 토큰
>    ```
>
> 3. **실험적 측정**
>    - 점진적으로 긴 텍스트를 입력하여 처리 한계 지점 확인
>    - 토큰 카운터 도구 사용 (tiktoken, cl100k_base 등)

### 5.3.2 2단계: 설계 원칙에 따른 에이전트 분리 결정

컨텍스트 윈도우라는 물리적 제약을 만족했다면, 이제 더 나은 설계를 위해 **단일 책임 원칙(SRP, Single Responsibility Principle)[^6]**을 적용하여 분리 여부를 결정합니다. 에이전트를 설계할 때 각 에이전트가 맡을 책임을 명확히 하기 위해 다음 세 가지 관점을 고려해보면 좋습니다.

#### 전문성 (Expertise)
책임들이 서로 다른 전문 지식을 요구하는가?

서로 다른 전문 영역의 책임은 별도 에이전트로 분리해야 합니다. 이는 각 에이전트가 명확한 역할과 책임을 갖게 하는 가장 직관적인 기준입니다.

- 예시 1: 'UI 디자인'과 '데이터베이스 스키마 설계'는 완전히 다른 전문성을 요구하므로, `UI 에이전트`와 `DB 에이전트`로 분리해야 합니다.
- 예시 2: '법률 검토'와 '재무 분석'은 서로 다른 전문 영역이므로, `법률 검토 에이전트`와 `재무 분석 에이전트`로 분리하는 것이 좋습니다.

#### 재사용성 (Reusability)
책임의 일부가 다른 워크플로우에서도 재사용될 수 있는가?

다양한 상황에서 재사용 가능한 책임은 독립적인 에이전트로 만들면 효율적입니다.

- 예: '코드 리뷰' 책임은 어떤 코드 생성 작업에서든 재사용될 수 있으므로, 독립적인 `코드 리뷰 에이전트`로 만드는 것이 효율적입니다.

#### 응집도 (Cohesion)[^7]
책임들이 '하나의 목적'을 위해 긴밀하게 묶여 있는가?

관련성이 높고 하나의 목적을 공유하는 책임들은 하나의 에이전트로 묶는 것이 좋습니다.

- 예: '버튼, 텍스트 필드, 드롭다운 메뉴 생성'은 모두 'UI 위젯 제작'이라는 단일 목적을 가지므로, `공통 위젯 에이전트` 하나로 묶는 것이 응집도가 높습니다.

## 실무 예제로 이어보기

지금까지 우리는 단일 작업을 수행하는 전문가, 즉 '워커 에이전트'를 설계하는 법을 배웠습니다. 이어지는 장들에서는 이러한 워커들을 지휘하여 복잡한 프로젝트를 관리하는 **'아키텍트 에이전트'**와, 심지어 이 아키텍트 시스템 자체를 설계하는 **'메타 에이전트'**의 개념으로 확장해 나갈 것입니다.

이 장에서 배운 개념들을 종합하여 실제 파일 기반 인스트럭션 시스템으로 구축하는 전체 과정은 [10장. 상황별 인스트럭션 설계 패턴 예제](10-1-single-agent-patterns.md)에서 자세히 다룹니다.

## 참고 자료

- Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401. https://arxiv.org/abs/2005.11401
- Liu, N. F., et al. (2023). Lost in the Middle: How Language Models Use Long Contexts. arXiv:2307.03172. https://arxiv.org/abs/2307.03172
- Martin, R. C. (2017). Clean Architecture: A Craftsman's Guide to Software Structure and Design. Prentice Hall.
- Anthropic. Claude 3/3.5 Models Overview (Context Window). https://docs.anthropic.com/claude/docs/models-overview
- OpenAI. Models and Context Lengths. https://platform.openai.com/docs/models
- Google. Gemini 1.5 Model Context Window. https://ai.google.dev/models/gemini

---

[^1]: **컨텍스트 윈도우(Context Window):** AI 모델이 한 번의 상호작용에서 처리하고 기억할 수 있는 최대 정보의 양. 토큰(Token) 단위로 측정되며, 이 크기를 넘어서는 정보는 모델이 인지하지 못할 수 있다.

[^2]: **Lost in the Middle:** 긴 컨텍스트(문맥)를 처리할 때, AI 모델이 입력의 시작이나 끝 부분이 아닌 중간에 위치한 정보를 잘 활용하지 못하거나 잊어버리는 경향을 설명하는 연구 결과.

[^3]: **할루시네이션(Hallucination):** AI 모델이 학습 데이터에 근거하지 않거나 사실과 다른 내용을, 마치 실제 사실인 것처럼 그럴듯하게 생성하는 현상. '의도치 않은 정보 날조' 또는 '그럴듯한 거짓말'이라고도 불린다.

[^4]: **RAG(Retrieval-Augmented Generation):** 대규모 언어 모델(LLM)이 답변을 생성할 때, 미리 준비된 외부 지식 베이스에서 관련된 정보를 실시간으로 검색(Retrieval)하고, 이 정보를 참고하여 답변 생성을 보강(Augmented Generation)하는 기술. 할루시네이션을 줄이고 정보의 최신성을 확보하는 데 효과적이다.

[^5]: **프롬프트 인젝션(Prompt Injection):** 사용자의 입력값에 악의적인 지시를 몰래 삽입하여, AI가 원래의 지시를 무시하고 공격자의 의도대로 작동하도록 만드는 보안 공격 기법.

[^6]: **단일 책임 원칙 (Single Responsibility Principle, SRP):** 하나의 모듈(여기서는 에이전트)은 단 하나의 액터(사용자 또는 이해관계자)에 대해서만 책임을 져야 한다는 객체지향 설계 원칙. 즉, 변경의 이유가 단 하나여야 함을 의미한다.

[^7]: **응집도 (Cohesion):** 하나의 모듈 내의 요소들이 얼마나 서로 밀접하게 관련되어 있는지를 나타내는 척도. 높은 응집도는 모듈이 단일한 목적을 위해 잘 설계되었음을 의미하며, 유지보수성과 재사용성을 높인다.

[^8]: **공식 문서(Anthropic, Claude 모델):** Claude 3/3.5 시리즈의 컨텍스트 윈도우는 최대 200K 토큰. 자세한 사양은 Anthropic 문서 참조.

[^9]: **공식 문서(OpenAI, GPT 계열):** GPT-4 Turbo(128K), GPT-4(8K/32K), GPT-3.5 Turbo(16K) 등 모델별 컨텍스트 길이는 OpenAI 문서의 모델 목록에서 확인 가능.

[^10]: **공식 문서(Google, Gemini 1.5):** Gemini 1.5 Pro(최대 약 2M), Gemini 1.5 Flash(최대 약 1M) 토큰 컨텍스트 윈도우. 세부 사양은 Google AI 문서 참조.
