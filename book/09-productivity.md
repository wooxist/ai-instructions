# 9장. 인스트럭션의 평가와 검증

**Part 2: 인스트럭션 시스템 설계와 평가**

**목적:** '감'에 의존하는 대신, 간단한 실험과 객관적인 기준을 통해 자신의 인스트럭션을 스스로 평가하고 지속적으로 개선하는 실용적인 방법론을 학습합니다.

### 이 장에서 배우는 것
- 인스트럭션 개선에 왜 객관적인 기준이 중요한지 이해합니다.
- AI의 결과물을 보고 '좋다/나쁘다'를 판단하는 나만의 평가 지표를 설정하는 법을 배웁니다.
- 두 가지 프롬프트를 비교하는 '나만의 A/B 테스트' 방법을 익힙니다.
- 전문가들이 사용하는 자동화된 평가 및 검증 시스템(Instruction DevOps)의 기본 개념을 참고용으로 알아봅니다.

## 9.1 왜 '감'이 아닌 '기준'이 필요할까?

8장에서 우리는 AI 사용의 효율을 높이는 여러 전략을 배웠습니다. 하지만 내가 수정한 인스트럭션이 정말로 '더 나아진 것'인지 어떻게 확신할 수 있을까요? 이는 마치 요리 실력을 늘리는 과정과 같습니다. 어제보다 더 맛있는 파스타를 만들기 위해 레시피를 조금 바꿨을 때, "왠지 더 맛있어진 것 같아"라는 '감'에만 의존한다면, 그 성공을 다음번에 재현하기 어렵습니다. "소금을 2g 더 넣고, 면을 30초 덜 삶았더니 맛의 균형이 잡혔다"처럼 구체적인 '기준'으로 기록해야만, 나만의 '필승 레시피'를 완성하고 꾸준히 발전시킬 수 있습니다.

인스트럭션 개선도 마찬가지입니다. '감'에 의존한 판단은 다음과 같은 문제를 만듭니다.
- **주관적 편향:** 나의 기대가 섞여 실제로는 나빠졌는데도 좋아졌다고 착각할 수 있습니다.
- **재현 불가능:** 왜 좋아졌는지 설명할 수 없어, 그 성공 경험을 다른 작업에 적용하기 어렵습니다.

따라서 '감'이 아닌 명확한 '기준'을 가지고 인스트럭션을 개선해야, 나의 AI 활용 능력을 꾸준히 성장시킬 수 있습니다. 이 장에서는 복잡한 시스템 없이, 일반 사용자도 자신의 인스트럭션을 체계적으로 개선하는 간단하고 실용적인 방법들을 소개합니다.

## 9.2 무엇을 보고 개선할까?: 나만의 평가 지표 찾기

인스트럭션을 개선하려면, 먼저 무엇이 '좋은 결과'인지 정의해야 합니다. 복잡한 측정 도구 없이도, 여러분이 AI와 상호작용하며 자연스럽게 느낄 수 있는 것들을 '나만의 평가 지표'로 삼을 수 있습니다.

- **1. 재작업 비율 (Rework Rate):** "AI의 결과물을 얼마나 많이 직접 수정해야 했는가?"
  - AI가 내놓은 결과물을 거의 그대로 사용할 수 있었다면 재작업 비율은 낮고, 품질은 높은 것입니다. 반면, 결과물의 대부분을 직접 다시 쓰거나 수정해야 했다면, 이는 프롬프트를 개선해야 한다는 강력한 신호입니다.

- **2. 주관적 만족도 점수 (Satisfaction Score):** "이 결과물이 얼마나 마음에 드는가?"
  - 8장에서 소개한 'AI 협업 로그'나 별도의 노트에, 결과물에 대한 만족도를 1~5점으로 간단히 기록해보세요. 어떤 종류의 프롬프트를 썼을 때 높은 점수가 나오는지 패턴을 발견하면, 그것이 바로 여러분의 '성공 공식'이 됩니다.

- **3. 아이디어 채택률 (Idea Adoption Rate):** "AI의 제안 중 실제 업무에 적용한 아이디어는 몇 개인가?"
  - 특히 브레인스토밍이나 기획 같은 창의적인 작업에서 유용한 지표입니다. AI가 10개의 아이디어를 냈는데 그중 5개를 회의에 가져갔다면, 채택률은 50%입니다. 이 비율을 높이는 방향으로 프롬프트를 수정해볼 수 있습니다.

> **[참고] 전문가의 지표**
> LLM을 활용해 서비스를 만드는 개발자들은 보다 엄격한 정량 지표를 사용합니다. 예를 들어, AI의 답변이 정답과 일치하는 비율인 **정확도(Accuracy)**, 요청부터 답변 완료까지 걸리는 시간인 **속도(Latency)**, 그리고 API 호출에 드는 **비용(Cost)** 등을 시스템으로 측정하여 평가합니다.

## 9.3 어떻게 개선할까?: 나만의 작은 실험실

나만의 평가 지표가 생겼다면, 이제 간단한 실험을 통해 인스트럭션을 개선할 차례입니다.

### 9.3.1 나만의 A/B 테스트: 더 나은 프롬프트 찾기

A/B 테스트는 두 가지 대안(A와 B) 중 어느 것이 더 나은지 비교하는 가장 확실한 방법입니다.

- **실험 방법:**
  1.  **두 개의 프롬프트 준비:** 기존 프롬프트(A)와 개선하고 싶은 새 프롬프트(B)를 준비합니다.
  2.  **두 개의 채팅창 활용:** ChatGPT와 같은 서비스에서 두 개의 새 채팅창을 엽니다. 하나의 창에는 A를, 다른 창에는 B를 입력하여 각각 결과물을 얻습니다. (반드시 '새 채팅'으로 시작해야 서로 영향을 주지 않습니다.)
  3.  **결과 비교:** 9.2에서 정한 '나만의 평가 지표' (재작업 비율, 만족도 등)를 기준으로 어떤 결과물이 더 나은지 객관적으로 판단합니다.

이 간단한 실험만으로도, 여러분은 더 효과적인 프롬프트 표현법, 더 유용한 제약 조건 등을 데이터에 기반하여 찾아낼 수 있습니다.

### 9.3.2 오답 노트 만들기: 실수로부터 배우기

'개선'이 '개악'으로 이어지는 경우를 막기 위한 안전장치입니다. 특정 문제를 해결하기 위해 프롬프트를 수정했더니, 기존에 잘 되던 다른 기능이 망가지는 것을 '회귀(Regression)'라고 합니다.

- **실천 방법:**
  1.  **실수 기록:** AI가 유독 실수를 자주 하거나, 결과물이 만족스럽지 않았던 작업(예: '이메일 초안의 격식 수준이 자꾸 틀림')을 '오답 노트'처럼 기록해 둡니다.
  2.  **수정 후 재확인:** 프롬프트를 수정한 후에는, 이 '오답 노트'에 적힌 작업들을 다시 요청해보세요. 수정된 프롬프트가 과거의 실수를 반복하지 않는지, 기존에 잘 되던 부분은 여전히 잘 작동하는지 확인하는 것입니다.

이 '오답 노트'는 여러분의 인스트럭션이 꾸준히 발전하고 있다는 신뢰를 주는 중요한 자산이 됩니다.

### 9.3.3 [참고] '평가 에이전트'를 활용한 자동화된 검증

고급 사용자나 팀에서는 앞서 설명한 수동 평가 과정을 자동화하는 '평가 에이전트'를 만들어 활용하기도 합니다. 이는 **하나의 자동화된 프로세스**이며, 인스트럭션의 품질을 꾸준히 관리하는 효과적인 방법입니다.

**'평가 에이전트'의 작동 방식:**

1.  **표준 질문 준비:** 평가에 사용할 표준 질문과 이상적인 답변 예시가 담긴 '평가 세트'를 미리 준비합니다.
2.  **자동 테스트 실행:** 테스트하고 싶은 여러 버전의 프롬프트(A, B, C...)에 대해, 준비된 모든 질문을 자동으로 실행하고 결과물을 수집합니다.
3.  **품질 자동 평가 (AI-as-a-Judge):** 수집된 결과물의 '품질'을 평가하기 위해, **또 다른 AI를 심판(Judge)으로 활용**합니다.
    - **작동 예시:** 심판 AI에게 "다음은 '이상적인 답변'과 'AI가 생성한 답변'이다. 생성된 답변이 이상적인 답변의 핵심 요구사항을 모두 충족하는지 평가하고, '매우 우수', '보통', '미흡' 중 하나로 점수를 부여하라." 와 같은 인스트럭션을 전달하여 품질을 점수화합니다.
4.  **리포트 생성:** 모든 테스트가 끝나면, "B 프롬프트는 A에 비해 '매우 우수' 등급을 15% 더 많이 받았다"와 같은 최종 비교 리포트를 생성합니다.

이러한 '평가 에이전트'를 구축하면, 프롬프트를 수정할 때마다 객관적인 데이터에 기반한 빠르고 정확한 의사결정을 내릴 수 있습니다.

## 9.4 정답이 있는 업무 vs. 정답이 없는 업무

지금까지의 논의는 '더 나은 결과'를 판단할 수 있는 업무에 초점을 맞추었습니다. 하지만 작업의 성격에 따라 평가의 접근법은 달라져야 합니다.

- **정답이 비교적 명확한 업무 (Structured Tasks):** 데이터 추출, 분류, 코드 수정 등 결과의 좋고 나쁨을 판단하기 쉬운 작업입니다. 이 경우, '재작업 비율'이나 '정확도' 같은 지표 중심의 평가가 효과적입니다.
- **정답이 없는 창의적 업무 (Creative Tasks):** 마케팅 슬로건 작성, 사업 전략 제안 등 정답이 없고 '취향'이나 '새로움'이 중요한 작업입니다. 이 경우, 여러 버전의 결과물을 놓고 "어떤 버전이 더 마음에 드시나요?"라고 동료에게 묻거나, '아이디어 채택률' 같은 지표를 활용하는 것이 더 의미 있습니다.

## 참고 자료

- Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. *arXiv preprint arXiv:1702.08608*.
- Kohavi, R., et al. (2009). Controlled experiments on the web: survey and practical guide. *Data mining and knowledge discovery*.
- Microsoft Research. (2023). Human-AI Guidelines. https://www.microsoft.com/en-us/research/project/guidelines-for-human-ai-interaction/
- Nielsen, J. (1994). *Usability engineering*. Morgan Kaufmann.

## 실습 체크리스트

### 이 장을 완료하셨다면 다음을 확인하세요:
- [ ] 나만의 평가 지표(재작업 비율, 만족도, 채택률 등)를 정의했다
- [ ] A/B 테스트 방법을 숙지하고 동일 과제의 두 버전을 공정하게 비교할 수 있다
- [ ] 회귀를 방지하기 위한 ‘오답 노트’ 운영 방식을 설계했다

### 실습 과제
1. 최근 사용한 인스트럭션 1개를 골라 A/B 실험을 설계·실행하고, 지표 2개로 비교하세요.
2. 실패 사례 3개를 ‘오답 노트’로 정리하고, 수정 후 회귀 여부를 재검증하세요.
